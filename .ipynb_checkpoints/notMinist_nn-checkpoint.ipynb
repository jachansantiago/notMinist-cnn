{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract('./notMNIST_large')\n",
    "test_folders = maybe_extract('./notMNIST_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (imageio.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except (IOError, ValueError) as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 1000\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_output = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(dtype=tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name=\"training\")\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    \n",
    "    hidden = tf.layers.dense(X_drop, n_hidden, name=\"hidden\")\n",
    "    bn = tf.layers.batch_normalization(hidden, training=training, momentum=0.9)\n",
    "    bn_act = tf.nn.elu(bn)\n",
    "    hidden_drop = tf.layers.dropout(bn_act, dropout_rate, training=training)\n",
    "    \n",
    "    hidden1 = tf.layers.dense(hidden_drop, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "    bn1_act = tf.nn.elu(bn1)\n",
    "    hidden1_drop = tf.layers.dropout(bn1_act, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "    bn2_act = tf.nn.elu(bn2)\n",
    "    hidden2_drop = tf.layers.dropout(bn2_act, dropout_rate, training=training)\n",
    "    \n",
    "    \n",
    "    logits_before_bn = tf.layers.dense(hidden2_drop, n_output, name=\"outputs\")\n",
    "    logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 784) (200000,)\n",
      "Validation: (10000, 784) (10000,)\n",
      "Testing: (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_labels.shape\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.reshape((train_dataset.shape[0], 28*28))\n",
    "test_dataset = test_dataset.reshape((test_dataset.shape[0], 28*28))\n",
    "valid_dataset = valid_dataset.reshape((valid_dataset.shape[0], 28*28))\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "#print(X_b.shape, X_b.dtype, X_b.min(), X_b.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.88 Test accuracy: 0.8736\n",
      "1 Train accuracy: 0.84 Test accuracy: 0.8832\n",
      "2 Train accuracy: 0.76 Test accuracy: 0.8821\n",
      "3 Train accuracy: 0.84 Test accuracy: 0.8886\n",
      "4 Train accuracy: 0.76 Test accuracy: 0.8953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-f2a5b301d78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_b_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0myi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_update_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0myi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/mlbook/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/mlbook/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/mlbook/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda/envs/mlbook/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/mlbook/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 250\n",
    "n_batches = 50\n",
    "batch_size = 50\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(train_size)\n",
    "        X_b_shuffled = train_dataset[shuffled_indices]\n",
    "        y_shuffled = train_labels[shuffled_indices]\n",
    "        \n",
    "        for i in range(0, train_size, batch_size):\n",
    "            xi = X_b_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={X:xi, y:yi, training:True})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X:xi, y:yi})\n",
    "        acc_test = accuracy.eval(feed_dict={X:test_dataset, y:test_labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./notMnist-dropouts.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-dropouts.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-dropouts.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.74 Test accuracy: 0.9182\n",
      "1 Train accuracy: 0.92 Test accuracy: 0.9198\n",
      "2 Train accuracy: 0.92 Test accuracy: 0.9194\n",
      "3 Train accuracy: 0.9 Test accuracy: 0.9181\n",
      "4 Train accuracy: 0.84 Test accuracy: 0.9196\n",
      "5 Train accuracy: 0.82 Test accuracy: 0.9196\n",
      "6 Train accuracy: 0.88 Test accuracy: 0.9199\n",
      "7 Train accuracy: 0.82 Test accuracy: 0.9204\n",
      "8 Train accuracy: 0.94 Test accuracy: 0.9204\n",
      "9 Train accuracy: 0.94 Test accuracy: 0.9214\n",
      "10 Train accuracy: 0.9 Test accuracy: 0.9212\n",
      "11 Train accuracy: 0.88 Test accuracy: 0.921\n",
      "12 Train accuracy: 0.88 Test accuracy: 0.9211\n",
      "13 Train accuracy: 0.88 Test accuracy: 0.9223\n",
      "14 Train accuracy: 0.86 Test accuracy: 0.9228\n",
      "15 Train accuracy: 0.88 Test accuracy: 0.9235\n",
      "16 Train accuracy: 0.84 Test accuracy: 0.9241\n",
      "17 Train accuracy: 0.82 Test accuracy: 0.9226\n",
      "18 Train accuracy: 0.82 Test accuracy: 0.9248\n",
      "19 Train accuracy: 0.9 Test accuracy: 0.9238\n",
      "20 Train accuracy: 0.92 Test accuracy: 0.9244\n",
      "21 Train accuracy: 0.78 Test accuracy: 0.9249\n",
      "22 Train accuracy: 0.86 Test accuracy: 0.9253\n",
      "23 Train accuracy: 0.8 Test accuracy: 0.9247\n",
      "24 Train accuracy: 0.82 Test accuracy: 0.9254\n",
      "25 Train accuracy: 0.84 Test accuracy: 0.926\n",
      "26 Train accuracy: 0.88 Test accuracy: 0.9265\n",
      "27 Train accuracy: 0.9 Test accuracy: 0.9255\n",
      "28 Train accuracy: 0.86 Test accuracy: 0.9257\n",
      "29 Train accuracy: 0.84 Test accuracy: 0.9275\n",
      "30 Train accuracy: 0.84 Test accuracy: 0.927\n",
      "31 Train accuracy: 0.82 Test accuracy: 0.9267\n",
      "32 Train accuracy: 0.84 Test accuracy: 0.9269\n",
      "33 Train accuracy: 0.82 Test accuracy: 0.9271\n",
      "34 Train accuracy: 0.92 Test accuracy: 0.9264\n",
      "35 Train accuracy: 0.86 Test accuracy: 0.9282\n",
      "36 Train accuracy: 0.94 Test accuracy: 0.928\n",
      "37 Train accuracy: 0.92 Test accuracy: 0.9277\n",
      "38 Train accuracy: 0.9 Test accuracy: 0.9269\n",
      "39 Train accuracy: 0.86 Test accuracy: 0.9277\n",
      "40 Train accuracy: 0.94 Test accuracy: 0.9281\n",
      "41 Train accuracy: 0.88 Test accuracy: 0.9292\n",
      "42 Train accuracy: 0.92 Test accuracy: 0.929\n",
      "43 Train accuracy: 0.94 Test accuracy: 0.9291\n",
      "44 Train accuracy: 0.88 Test accuracy: 0.9289\n",
      "45 Train accuracy: 0.8 Test accuracy: 0.9293\n",
      "46 Train accuracy: 0.84 Test accuracy: 0.929\n",
      "47 Train accuracy: 0.9 Test accuracy: 0.9301\n",
      "48 Train accuracy: 0.92 Test accuracy: 0.93\n",
      "49 Train accuracy: 0.86 Test accuracy: 0.93\n",
      "50 Train accuracy: 0.82 Test accuracy: 0.9312\n",
      "51 Train accuracy: 0.9 Test accuracy: 0.9305\n",
      "52 Train accuracy: 0.92 Test accuracy: 0.9315\n",
      "53 Train accuracy: 0.94 Test accuracy: 0.9307\n",
      "54 Train accuracy: 0.88 Test accuracy: 0.9311\n",
      "55 Train accuracy: 0.82 Test accuracy: 0.932\n",
      "56 Train accuracy: 0.82 Test accuracy: 0.9308\n",
      "57 Train accuracy: 0.84 Test accuracy: 0.9333\n",
      "58 Train accuracy: 0.82 Test accuracy: 0.9324\n",
      "59 Train accuracy: 0.96 Test accuracy: 0.9326\n",
      "60 Train accuracy: 0.98 Test accuracy: 0.932\n",
      "61 Train accuracy: 0.94 Test accuracy: 0.932\n",
      "62 Train accuracy: 0.98 Test accuracy: 0.9327\n",
      "63 Train accuracy: 0.88 Test accuracy: 0.9323\n",
      "64 Train accuracy: 0.92 Test accuracy: 0.9328\n",
      "65 Train accuracy: 0.92 Test accuracy: 0.9338\n",
      "66 Train accuracy: 0.9 Test accuracy: 0.9323\n",
      "67 Train accuracy: 0.9 Test accuracy: 0.9316\n",
      "68 Train accuracy: 0.82 Test accuracy: 0.9339\n",
      "69 Train accuracy: 0.88 Test accuracy: 0.9322\n",
      "70 Train accuracy: 0.88 Test accuracy: 0.9334\n",
      "71 Train accuracy: 0.88 Test accuracy: 0.9322\n",
      "72 Train accuracy: 0.98 Test accuracy: 0.9348\n",
      "73 Train accuracy: 0.92 Test accuracy: 0.9326\n",
      "74 Train accuracy: 0.9 Test accuracy: 0.9339\n",
      "75 Train accuracy: 0.84 Test accuracy: 0.9344\n",
      "76 Train accuracy: 0.96 Test accuracy: 0.9359\n",
      "77 Train accuracy: 0.96 Test accuracy: 0.9347\n",
      "78 Train accuracy: 0.88 Test accuracy: 0.9344\n",
      "79 Train accuracy: 0.86 Test accuracy: 0.9348\n",
      "80 Train accuracy: 0.86 Test accuracy: 0.935\n",
      "81 Train accuracy: 0.86 Test accuracy: 0.9345\n",
      "82 Train accuracy: 0.8 Test accuracy: 0.9353\n",
      "83 Train accuracy: 0.9 Test accuracy: 0.9354\n",
      "84 Train accuracy: 0.9 Test accuracy: 0.9352\n",
      "85 Train accuracy: 0.84 Test accuracy: 0.9345\n",
      "86 Train accuracy: 0.88 Test accuracy: 0.9347\n",
      "87 Train accuracy: 0.9 Test accuracy: 0.9357\n",
      "88 Train accuracy: 0.94 Test accuracy: 0.9371\n",
      "89 Train accuracy: 0.84 Test accuracy: 0.9357\n",
      "90 Train accuracy: 0.9 Test accuracy: 0.9372\n",
      "91 Train accuracy: 0.9 Test accuracy: 0.9365\n",
      "92 Train accuracy: 0.8 Test accuracy: 0.9365\n",
      "93 Train accuracy: 0.96 Test accuracy: 0.9369\n",
      "94 Train accuracy: 0.82 Test accuracy: 0.9359\n",
      "95 Train accuracy: 0.88 Test accuracy: 0.9359\n",
      "96 Train accuracy: 0.82 Test accuracy: 0.9377\n",
      "97 Train accuracy: 0.94 Test accuracy: 0.9361\n",
      "98 Train accuracy: 0.94 Test accuracy: 0.9364\n",
      "99 Train accuracy: 0.9 Test accuracy: 0.9362\n",
      "100 Train accuracy: 0.94 Test accuracy: 0.9372\n",
      "101 Train accuracy: 0.86 Test accuracy: 0.936\n",
      "102 Train accuracy: 0.88 Test accuracy: 0.936\n",
      "103 Train accuracy: 0.98 Test accuracy: 0.9375\n",
      "104 Train accuracy: 0.84 Test accuracy: 0.9398\n",
      "105 Train accuracy: 0.88 Test accuracy: 0.9355\n",
      "106 Train accuracy: 0.92 Test accuracy: 0.937\n",
      "107 Train accuracy: 0.86 Test accuracy: 0.9364\n",
      "108 Train accuracy: 0.9 Test accuracy: 0.9391\n",
      "109 Train accuracy: 0.96 Test accuracy: 0.9369\n",
      "110 Train accuracy: 0.92 Test accuracy: 0.9373\n",
      "111 Train accuracy: 0.78 Test accuracy: 0.9378\n",
      "112 Train accuracy: 0.86 Test accuracy: 0.9387\n",
      "113 Train accuracy: 0.86 Test accuracy: 0.9373\n",
      "114 Train accuracy: 0.92 Test accuracy: 0.9387\n",
      "115 Train accuracy: 0.9 Test accuracy: 0.9392\n",
      "116 Train accuracy: 0.84 Test accuracy: 0.9391\n",
      "117 Train accuracy: 0.8 Test accuracy: 0.9385\n",
      "118 Train accuracy: 0.96 Test accuracy: 0.9391\n",
      "119 Train accuracy: 0.9 Test accuracy: 0.9383\n",
      "120 Train accuracy: 0.86 Test accuracy: 0.9386\n",
      "121 Train accuracy: 0.86 Test accuracy: 0.9385\n",
      "122 Train accuracy: 0.86 Test accuracy: 0.9389\n",
      "123 Train accuracy: 0.92 Test accuracy: 0.9405\n",
      "124 Train accuracy: 0.9 Test accuracy: 0.94\n",
      "125 Train accuracy: 0.88 Test accuracy: 0.94\n",
      "126 Train accuracy: 0.96 Test accuracy: 0.9386\n",
      "127 Train accuracy: 0.86 Test accuracy: 0.9386\n",
      "128 Train accuracy: 0.88 Test accuracy: 0.9402\n",
      "129 Train accuracy: 0.86 Test accuracy: 0.9404\n",
      "130 Train accuracy: 0.96 Test accuracy: 0.94\n",
      "131 Train accuracy: 0.88 Test accuracy: 0.9394\n",
      "132 Train accuracy: 0.88 Test accuracy: 0.9384\n",
      "133 Train accuracy: 0.86 Test accuracy: 0.9382\n",
      "134 Train accuracy: 0.86 Test accuracy: 0.9397\n",
      "135 Train accuracy: 0.92 Test accuracy: 0.9391\n",
      "136 Train accuracy: 0.88 Test accuracy: 0.9391\n",
      "137 Train accuracy: 0.86 Test accuracy: 0.9402\n",
      "138 Train accuracy: 0.92 Test accuracy: 0.9401\n",
      "139 Train accuracy: 0.9 Test accuracy: 0.9389\n",
      "140 Train accuracy: 0.94 Test accuracy: 0.9405\n",
      "141 Train accuracy: 0.88 Test accuracy: 0.9395\n",
      "142 Train accuracy: 0.8 Test accuracy: 0.9409\n",
      "143 Train accuracy: 0.9 Test accuracy: 0.9408\n",
      "144 Train accuracy: 0.9 Test accuracy: 0.94\n",
      "145 Train accuracy: 0.88 Test accuracy: 0.941\n",
      "146 Train accuracy: 0.84 Test accuracy: 0.9399\n",
      "147 Train accuracy: 0.9 Test accuracy: 0.939\n",
      "148 Train accuracy: 0.9 Test accuracy: 0.941\n",
      "149 Train accuracy: 0.92 Test accuracy: 0.9396\n",
      "150 Train accuracy: 0.82 Test accuracy: 0.942\n",
      "151 Train accuracy: 0.86 Test accuracy: 0.9411\n",
      "152 Train accuracy: 0.96 Test accuracy: 0.9422\n",
      "153 Train accuracy: 0.86 Test accuracy: 0.9414\n",
      "154 Train accuracy: 0.92 Test accuracy: 0.941\n",
      "155 Train accuracy: 0.9 Test accuracy: 0.9413\n",
      "156 Train accuracy: 0.96 Test accuracy: 0.9417\n",
      "157 Train accuracy: 0.88 Test accuracy: 0.9413\n",
      "158 Train accuracy: 0.92 Test accuracy: 0.9418\n",
      "159 Train accuracy: 0.9 Test accuracy: 0.9406\n",
      "160 Train accuracy: 0.84 Test accuracy: 0.9412\n",
      "161 Train accuracy: 0.94 Test accuracy: 0.9414\n",
      "162 Train accuracy: 0.92 Test accuracy: 0.9414\n",
      "163 Train accuracy: 0.82 Test accuracy: 0.9417\n",
      "164 Train accuracy: 0.88 Test accuracy: 0.9424\n",
      "165 Train accuracy: 0.88 Test accuracy: 0.9425\n",
      "166 Train accuracy: 0.9 Test accuracy: 0.9413\n",
      "167 Train accuracy: 0.76 Test accuracy: 0.9423\n",
      "168 Train accuracy: 0.9 Test accuracy: 0.9413\n",
      "169 Train accuracy: 0.96 Test accuracy: 0.9424\n",
      "170 Train accuracy: 0.96 Test accuracy: 0.9427\n",
      "171 Train accuracy: 0.92 Test accuracy: 0.941\n",
      "172 Train accuracy: 0.94 Test accuracy: 0.9422\n",
      "173 Train accuracy: 0.88 Test accuracy: 0.9427\n",
      "174 Train accuracy: 0.96 Test accuracy: 0.9415\n",
      "175 Train accuracy: 0.92 Test accuracy: 0.9419\n",
      "176 Train accuracy: 0.84 Test accuracy: 0.9419\n",
      "177 Train accuracy: 0.94 Test accuracy: 0.9421\n",
      "178 Train accuracy: 0.82 Test accuracy: 0.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 Train accuracy: 0.94 Test accuracy: 0.9425\n",
      "180 Train accuracy: 0.88 Test accuracy: 0.9421\n",
      "181 Train accuracy: 0.84 Test accuracy: 0.9433\n",
      "182 Train accuracy: 0.9 Test accuracy: 0.943\n",
      "183 Train accuracy: 0.92 Test accuracy: 0.9429\n",
      "184 Train accuracy: 0.84 Test accuracy: 0.9425\n",
      "185 Train accuracy: 0.9 Test accuracy: 0.9446\n",
      "186 Train accuracy: 0.92 Test accuracy: 0.9442\n",
      "187 Train accuracy: 0.98 Test accuracy: 0.9436\n",
      "188 Train accuracy: 0.88 Test accuracy: 0.9423\n",
      "189 Train accuracy: 0.92 Test accuracy: 0.9439\n",
      "190 Train accuracy: 0.92 Test accuracy: 0.9439\n",
      "191 Train accuracy: 0.92 Test accuracy: 0.9431\n",
      "192 Train accuracy: 0.92 Test accuracy: 0.9437\n",
      "193 Train accuracy: 0.92 Test accuracy: 0.9429\n",
      "194 Train accuracy: 0.84 Test accuracy: 0.9442\n",
      "195 Train accuracy: 0.94 Test accuracy: 0.9437\n",
      "196 Train accuracy: 0.92 Test accuracy: 0.9426\n",
      "197 Train accuracy: 0.98 Test accuracy: 0.9429\n",
      "198 Train accuracy: 0.98 Test accuracy: 0.9438\n",
      "199 Train accuracy: 0.92 Test accuracy: 0.9423\n",
      "200 Train accuracy: 0.92 Test accuracy: 0.9451\n",
      "201 Train accuracy: 0.92 Test accuracy: 0.9434\n",
      "202 Train accuracy: 0.88 Test accuracy: 0.9427\n",
      "203 Train accuracy: 0.94 Test accuracy: 0.9447\n",
      "204 Train accuracy: 0.92 Test accuracy: 0.9448\n",
      "205 Train accuracy: 0.8 Test accuracy: 0.9442\n",
      "206 Train accuracy: 0.86 Test accuracy: 0.944\n",
      "207 Train accuracy: 0.86 Test accuracy: 0.9449\n",
      "208 Train accuracy: 0.96 Test accuracy: 0.9446\n",
      "209 Train accuracy: 0.9 Test accuracy: 0.9435\n",
      "210 Train accuracy: 0.88 Test accuracy: 0.943\n",
      "211 Train accuracy: 0.84 Test accuracy: 0.9439\n",
      "212 Train accuracy: 0.88 Test accuracy: 0.9431\n",
      "213 Train accuracy: 0.92 Test accuracy: 0.9441\n",
      "214 Train accuracy: 0.88 Test accuracy: 0.9455\n",
      "215 Train accuracy: 0.82 Test accuracy: 0.9424\n",
      "216 Train accuracy: 0.9 Test accuracy: 0.9438\n",
      "217 Train accuracy: 0.9 Test accuracy: 0.9436\n",
      "218 Train accuracy: 0.94 Test accuracy: 0.9453\n",
      "219 Train accuracy: 0.92 Test accuracy: 0.9453\n",
      "220 Train accuracy: 0.94 Test accuracy: 0.9444\n",
      "221 Train accuracy: 0.86 Test accuracy: 0.9452\n",
      "222 Train accuracy: 0.88 Test accuracy: 0.9456\n",
      "223 Train accuracy: 0.84 Test accuracy: 0.9443\n",
      "224 Train accuracy: 0.96 Test accuracy: 0.9452\n",
      "225 Train accuracy: 0.96 Test accuracy: 0.9456\n",
      "226 Train accuracy: 0.9 Test accuracy: 0.9473\n",
      "227 Train accuracy: 0.88 Test accuracy: 0.9439\n",
      "228 Train accuracy: 0.94 Test accuracy: 0.9448\n",
      "229 Train accuracy: 0.92 Test accuracy: 0.9438\n",
      "230 Train accuracy: 0.9 Test accuracy: 0.9439\n",
      "231 Train accuracy: 0.9 Test accuracy: 0.9457\n",
      "232 Train accuracy: 0.9 Test accuracy: 0.9449\n",
      "233 Train accuracy: 0.82 Test accuracy: 0.9446\n",
      "234 Train accuracy: 0.86 Test accuracy: 0.9456\n",
      "235 Train accuracy: 0.88 Test accuracy: 0.9453\n",
      "236 Train accuracy: 0.82 Test accuracy: 0.946\n",
      "237 Train accuracy: 0.88 Test accuracy: 0.9448\n",
      "238 Train accuracy: 0.9 Test accuracy: 0.9453\n",
      "239 Train accuracy: 0.84 Test accuracy: 0.9457\n",
      "240 Train accuracy: 0.74 Test accuracy: 0.945\n",
      "241 Train accuracy: 0.88 Test accuracy: 0.9461\n",
      "242 Train accuracy: 0.9 Test accuracy: 0.9461\n",
      "243 Train accuracy: 0.9 Test accuracy: 0.9462\n",
      "244 Train accuracy: 0.94 Test accuracy: 0.9455\n",
      "245 Train accuracy: 0.9 Test accuracy: 0.9461\n",
      "246 Train accuracy: 0.84 Test accuracy: 0.9447\n",
      "247 Train accuracy: 0.98 Test accuracy: 0.9452\n",
      "248 Train accuracy: 0.88 Test accuracy: 0.9458\n",
      "249 Train accuracy: 0.86 Test accuracy: 0.9454\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./notMnist-dropouts.ckpt\") # or better, use save_path\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(train_size)\n",
    "        X_b_shuffled = train_dataset[shuffled_indices]\n",
    "        y_shuffled = train_labels[shuffled_indices]\n",
    "        \n",
    "        for i in range(0, train_size, batch_size):\n",
    "            xi = X_b_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={X:xi, y:yi, training:True})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X:xi, y:yi})\n",
    "        acc_test = accuracy.eval(feed_dict={X:test_dataset, y:test_labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./notMnist-dropouts.ckpt\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-dropouts.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-dropouts.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./notMnist-dropouts.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: valid_dataset})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy : \", accuracy_score(y_pred, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[894  16   7  15   9  12  11  16   9  13]\n",
      " [  5 872  10  14  16   7  13   6  18   5]\n",
      " [  2   6 898   3  28   3  25   4   8   5]\n",
      " [ 24  24   6 913   9   4   8   8  23  10]\n",
      " [  2   9  13   3 852   8   9   3   6   6]\n",
      " [  8   6   7   7  13 913   5   7   7   9]\n",
      " [  9  10  23   4  25  11 883   5   4   7]\n",
      " [ 30  18  10  12  15   9  15 925  20  13]\n",
      " [ 19  36  21  16  30  21  19  24 849  34]\n",
      " [  7   3   5  13   3  12  12   2  56 898]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_pred, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (y_pred != valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = valid_dataset[index]\n",
    "pred = y_pred[index]\n",
    "lab = valid_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 28, 28)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = img.reshape((img.shape[0], 28, 28))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x108d88c88>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAACPCAYAAAARM4LLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUpzwX3BOpxgFW2FsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108234be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w_i = 42\n",
    "\n",
    "print(pred[w_i], lab[w_i])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(img[w_i], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
