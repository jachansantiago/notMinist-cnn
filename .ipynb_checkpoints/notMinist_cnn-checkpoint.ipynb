{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# GET timestamp to display\n",
    "def display_time():\n",
    "    t = datetime.now()\n",
    "    return t.strftime(\"[ %I:%M:%S %p %D ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large already present - Skipping extraction of ./notMNIST_large.\n",
      "['./notMNIST_large/A', './notMNIST_large/B', './notMNIST_large/C', './notMNIST_large/D', './notMNIST_large/E', './notMNIST_large/F', './notMNIST_large/G', './notMNIST_large/H', './notMNIST_large/I', './notMNIST_large/J']\n",
      "./notMNIST_small already present - Skipping extraction of ./notMNIST_small.\n",
      "['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D', './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', './notMNIST_small/I', './notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract('./notMNIST_large')\n",
    "test_folders = maybe_extract('./notMNIST_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "./notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (imageio.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except (IOError, ValueError) as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28, 1) (200000,)\n",
      "Validation: (10000, 28, 28, 1) (10000,)\n",
      "Testing: (10000, 28, 28, 1) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.reshape((train_dataset.shape[0], train_dataset.shape[1],\n",
    "                       train_dataset.shape[2], 1))\n",
    "\n",
    "test_dataset = test_dataset.reshape((test_dataset.shape[0], test_dataset.shape[1],\n",
    "                       test_dataset.shape[2], 1))\n",
    "\n",
    "valid_dataset = valid_dataset.reshape((valid_dataset.shape[0], valid_dataset.shape[1],\n",
    "                       valid_dataset.shape[2], 1))\n",
    "\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "\n",
    "conv1_fmaps = 32\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\"\n",
    "\n",
    "conv2_fmaps = 64\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\"\n",
    "\n",
    "pool3_fmaps = conv2_fmaps\n",
    "\n",
    "n_fc1 = 4096\n",
    "n_fc2 = 1024\n",
    "n_outputs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"conv_layers\"):\n",
    "    conv1 = tf.layers.conv2d(X, filters=conv1_fmaps, \n",
    "                            kernel_size=conv1_ksize, \n",
    "                            strides=conv1_stride,\n",
    "                            padding=conv1_pad,\n",
    "                            activation=tf.nn.relu,\n",
    "                            name=\"conv1\")\n",
    "\n",
    "    conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, \n",
    "                            kernel_size=conv2_ksize, \n",
    "                            strides=conv2_stride,\n",
    "                            padding=conv2_pad,\n",
    "                            activation=tf.nn.relu,\n",
    "                            name=\"conv2\")\n",
    "\n",
    "    pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"VALID\")\n",
    "    pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "#X = tf.layers.dropout(pool3_flat, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"fully_connected\"):\n",
    "    hidden1 = tf.layers.dense(pool3_flat, n_fc1, \n",
    "                             activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden_dp = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    \n",
    "    hidden2 = tf.layers.dense(hidden_dp, n_fc2, \n",
    "                             activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_dp = tf.layers.dropout(hidden_dp, dropout_rate, training=training)\n",
    "    \n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(hidden2_dp, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.85 Test accuracy: 0.8973\n",
      "1 Train accuracy: 0.88 Test accuracy: 0.9111\n",
      "2 Train accuracy: 0.88 Test accuracy: 0.9247\n",
      "3 Train accuracy: 0.94 Test accuracy: 0.9328\n",
      "4 Train accuracy: 0.93 Test accuracy: 0.938\n",
      "5 Train accuracy: 0.81 Test accuracy: 0.9428\n",
      "6 Train accuracy: 0.9 Test accuracy: 0.9476\n",
      "7 Train accuracy: 0.91 Test accuracy: 0.9495\n",
      "8 Train accuracy: 0.92 Test accuracy: 0.9521\n",
      "9 Train accuracy: 0.87 Test accuracy: 0.953\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "train_size_batch = 200000\n",
    "\n",
    "\n",
    "#extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(train_size)\n",
    "        X_b_shuffled = train_dataset[shuffled_indices]\n",
    "        y_shuffled = train_labels[shuffled_indices]\n",
    "        \n",
    "        for i in range(0, train_size_batch, batch_size):\n",
    "            xi = X_b_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            sess.run(training_op, feed_dict={X:xi, y:yi, training : True})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X:xi, y:yi})\n",
    "        acc_test = accuracy.eval(feed_dict={X:test_dataset, y:test_labels})\n",
    "        acc_valid = accuracy.eval(feed_dict={X:valid_dataset, y:valid_labels})\n",
    "        print( display_time(), epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Validate accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./notMnist-4096-cnn-dp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-4096-cnn-dp.ckpt\n",
      "[ 02:15:28 AM 01/06/18 ] - START\n",
      "[ 02:25:52 AM 01/06/18 ] 0 Train accuracy: 1.0 Test accuracy: 0.9751 Validate accuracy: 0.9302\n",
      "[ 02:36:17 AM 01/06/18 ] 1 Train accuracy: 1.0 Test accuracy: 0.9758 Validate accuracy: 0.9309\n",
      "[ 02:46:41 AM 01/06/18 ] 2 Train accuracy: 0.995 Test accuracy: 0.9754 Validate accuracy: 0.9309\n",
      "[ 02:57:03 AM 01/06/18 ] 3 Train accuracy: 0.995 Test accuracy: 0.9755 Validate accuracy: 0.9298\n",
      "[ 03:07:29 AM 01/06/18 ] 4 Train accuracy: 0.99 Test accuracy: 0.9757 Validate accuracy: 0.9301\n",
      "[ 03:17:53 AM 01/06/18 ] 5 Train accuracy: 0.995 Test accuracy: 0.9753 Validate accuracy: 0.9307\n",
      "[ 03:28:17 AM 01/06/18 ] 6 Train accuracy: 1.0 Test accuracy: 0.9752 Validate accuracy: 0.9309\n",
      "[ 03:38:41 AM 01/06/18 ] 7 Train accuracy: 0.985 Test accuracy: 0.9747 Validate accuracy: 0.9292\n",
      "[ 03:49:05 AM 01/06/18 ] 8 Train accuracy: 1.0 Test accuracy: 0.9756 Validate accuracy: 0.9301\n",
      "[ 03:59:30 AM 01/06/18 ] 9 Train accuracy: 0.995 Test accuracy: 0.9759 Validate accuracy: 0.9305\n",
      "[ 04:09:55 AM 01/06/18 ] 10 Train accuracy: 1.0 Test accuracy: 0.9754 Validate accuracy: 0.9306\n",
      "[ 04:20:19 AM 01/06/18 ] 11 Train accuracy: 1.0 Test accuracy: 0.9759 Validate accuracy: 0.9308\n",
      "[ 04:30:44 AM 01/06/18 ] 12 Train accuracy: 0.995 Test accuracy: 0.9759 Validate accuracy: 0.9306\n",
      "[ 04:41:08 AM 01/06/18 ] 13 Train accuracy: 1.0 Test accuracy: 0.976 Validate accuracy: 0.9306\n",
      "[ 04:51:32 AM 01/06/18 ] 14 Train accuracy: 0.99 Test accuracy: 0.9755 Validate accuracy: 0.9299\n",
      "[ 05:02:00 AM 01/06/18 ] 15 Train accuracy: 0.99 Test accuracy: 0.9757 Validate accuracy: 0.9306\n",
      "[ 05:12:24 AM 01/06/18 ] 16 Train accuracy: 0.995 Test accuracy: 0.9759 Validate accuracy: 0.9309\n",
      "[ 05:22:44 AM 01/06/18 ] 17 Train accuracy: 1.0 Test accuracy: 0.9759 Validate accuracy: 0.9308\n",
      "[ 05:33:08 AM 01/06/18 ] 18 Train accuracy: 0.99 Test accuracy: 0.9759 Validate accuracy: 0.9306\n",
      "[ 05:43:30 AM 01/06/18 ] 19 Train accuracy: 1.0 Test accuracy: 0.9768 Validate accuracy: 0.931\n",
      "[ 05:53:51 AM 01/06/18 ] 20 Train accuracy: 0.995 Test accuracy: 0.9764 Validate accuracy: 0.9319\n",
      "[ 06:04:14 AM 01/06/18 ] 21 Train accuracy: 0.995 Test accuracy: 0.9755 Validate accuracy: 0.9307\n",
      "[ 06:14:37 AM 01/06/18 ] 22 Train accuracy: 1.0 Test accuracy: 0.976 Validate accuracy: 0.9312\n",
      "[ 06:25:02 AM 01/06/18 ] 23 Train accuracy: 0.995 Test accuracy: 0.976 Validate accuracy: 0.9303\n",
      "[ 06:35:25 AM 01/06/18 ] 24 Train accuracy: 1.0 Test accuracy: 0.9763 Validate accuracy: 0.9304\n",
      "[ 06:45:49 AM 01/06/18 ] 25 Train accuracy: 0.985 Test accuracy: 0.9764 Validate accuracy: 0.9305\n",
      "[ 06:56:13 AM 01/06/18 ] 26 Train accuracy: 0.985 Test accuracy: 0.9761 Validate accuracy: 0.9305\n",
      "[ 07:06:38 AM 01/06/18 ] 27 Train accuracy: 1.0 Test accuracy: 0.9759 Validate accuracy: 0.9321\n",
      "[ 07:17:03 AM 01/06/18 ] 28 Train accuracy: 0.99 Test accuracy: 0.9769 Validate accuracy: 0.931\n",
      "[ 07:27:26 AM 01/06/18 ] 29 Train accuracy: 1.0 Test accuracy: 0.9761 Validate accuracy: 0.931\n",
      "[ 07:37:51 AM 01/06/18 ] 30 Train accuracy: 0.995 Test accuracy: 0.9766 Validate accuracy: 0.9313\n",
      "[ 07:48:13 AM 01/06/18 ] 31 Train accuracy: 0.995 Test accuracy: 0.9765 Validate accuracy: 0.9307\n",
      "[ 07:58:39 AM 01/06/18 ] 32 Train accuracy: 0.995 Test accuracy: 0.976 Validate accuracy: 0.931\n",
      "[ 08:09:02 AM 01/06/18 ] 33 Train accuracy: 0.99 Test accuracy: 0.9767 Validate accuracy: 0.9307\n",
      "[ 08:19:26 AM 01/06/18 ] 34 Train accuracy: 1.0 Test accuracy: 0.9762 Validate accuracy: 0.9318\n",
      "[ 08:29:50 AM 01/06/18 ] 35 Train accuracy: 0.99 Test accuracy: 0.9767 Validate accuracy: 0.9318\n",
      "[ 08:40:12 AM 01/06/18 ] 36 Train accuracy: 1.0 Test accuracy: 0.9764 Validate accuracy: 0.9309\n",
      "[ 08:50:44 AM 01/06/18 ] 37 Train accuracy: 0.995 Test accuracy: 0.9759 Validate accuracy: 0.9313\n",
      "[ 09:01:19 AM 01/06/18 ] 38 Train accuracy: 1.0 Test accuracy: 0.9761 Validate accuracy: 0.9306\n",
      "[ 09:11:57 AM 01/06/18 ] 39 Train accuracy: 0.995 Test accuracy: 0.9764 Validate accuracy: 0.9301\n",
      "[ 09:22:33 AM 01/06/18 ] 40 Train accuracy: 1.0 Test accuracy: 0.976 Validate accuracy: 0.9308\n",
      "[ 09:33:10 AM 01/06/18 ] 41 Train accuracy: 1.0 Test accuracy: 0.9759 Validate accuracy: 0.9313\n",
      "[ 09:43:48 AM 01/06/18 ] 42 Train accuracy: 0.995 Test accuracy: 0.9761 Validate accuracy: 0.931\n",
      "[ 09:54:26 AM 01/06/18 ] 43 Train accuracy: 0.995 Test accuracy: 0.9763 Validate accuracy: 0.9304\n",
      "[ 10:05:04 AM 01/06/18 ] 44 Train accuracy: 0.995 Test accuracy: 0.9757 Validate accuracy: 0.9313\n",
      "[ 10:15:43 AM 01/06/18 ] 45 Train accuracy: 1.0 Test accuracy: 0.9757 Validate accuracy: 0.9314\n",
      "[ 10:26:26 AM 01/06/18 ] 46 Train accuracy: 0.995 Test accuracy: 0.9757 Validate accuracy: 0.9306\n",
      "[ 10:37:13 AM 01/06/18 ] 47 Train accuracy: 0.995 Test accuracy: 0.9762 Validate accuracy: 0.931\n",
      "[ 10:47:51 AM 01/06/18 ] 48 Train accuracy: 1.0 Test accuracy: 0.9758 Validate accuracy: 0.9308\n",
      "[ 10:58:31 AM 01/06/18 ] 49 Train accuracy: 0.995 Test accuracy: 0.9767 Validate accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 200\n",
    "train_size_batch = 200000\n",
    "\n",
    "#extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./notMnist-4096-cnn-dp.ckpt\") # or better, use save_path\n",
    "    print(display_time(), \"- START\")\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_indices = np.random.permutation(train_size)\n",
    "        X_b_shuffled = train_dataset[shuffled_indices]\n",
    "        y_shuffled = train_labels[shuffled_indices]\n",
    "        \n",
    "        for i in range(0, train_size_batch , batch_size):\n",
    "            xi = X_b_shuffled[i:i+batch_size]\n",
    "            yi = y_shuffled[i:i+batch_size]\n",
    "            sess.run(training_op, feed_dict={X:xi, y:yi, training: True})\n",
    "        \n",
    "        acc_train = accuracy.eval(feed_dict={X:xi, y:yi})\n",
    "        acc_test = accuracy.eval(feed_dict={X:test_dataset, y:test_labels})\n",
    "        acc_valid = accuracy.eval(feed_dict={X:valid_dataset, y:valid_labels})\n",
    "        print(display_time(), epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test, \"Validate accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./notMnist-4096-cnn-dp.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./notMnist-4096-cnn-dp.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./notMnist-4096-cnn-dp.ckpt\") # or better, use save_path\n",
    "    Z = logits.eval(feed_dict={X: valid_dataset})\n",
    "    y_pred = np.argmax(Z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy : \", accuracy_score(y_pred, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[934   2   4   7   2  12   6   6   8   6]\n",
      " [  3 930   4  11  10   2   8   9  13   4]\n",
      " [  4   5 927   8  14   3  15   1   3   5]\n",
      " [ 11  16   2 942  10   3   2   4  10   9]\n",
      " [  3  11  15   5 925  12   3   6  10   6]\n",
      " [  6   8   2   2   6 933   5   6   5   5]\n",
      " [  7   8  26   4  13  13 937   6  11   8]\n",
      " [ 12   8   6   4   7   6   8 949   7   6]\n",
      " [ 12   3   9   9   4  12   9  10 912  30]\n",
      " [  8   9   5   8   9   4   7   3  21 921]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_pred, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
